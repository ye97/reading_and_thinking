---
title: cnn理解
categories:
  - 论文
  - 深度
  
description: <read more ...>
mathjax: true
permalink_defaults: 'category/:title/'
date: 2021-07-10 15:45:22
urlname:
tags:
---

# 什么是神经网络

人工神经网络（artificial neural network，ANN），简称神经网络（neural network，NN），是一种模仿生物神经网络的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具，常用来对输入和输出间复杂的关系进行建模，或用来探索数据的模式。

神经网络由神经元、节点与节点之间的连接（突触）所构成，如下图：  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-0858f09fa244d3220e988ed4bb4f77b9.png)  
每个神经网络单元抽象出来的数学模型如下，也叫感知器，它接收多个输入（x1，x2，x3…），产生一个输出，这就好比是神经末梢感受各种外部环境的变化（外部刺激），然后产生电信号，以便于转导到神经细胞（又叫神经元）。  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-a469fd492e5f6b4c6e9fbfc615f0839c.png)  
单个的感知器就构成了一个简单的模型，但在现实世界中，实际的决策模型则要复杂得多，往往是由多个感知器组成的多层网络，如下图所示，这也是经典的神经网络模型，由输入层、隐含层、输出层构成。  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-a6187f0034f4a0710b888f442e4fc5b3.png)  
人工神经网络可以映射任意复杂的非线性关系，具有很强的鲁棒性、记忆能力、自学习等能力，在分类、预测、模式识别等方面有着广泛的应用。

# 什么是卷积神经网络

卷积神经网络（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。  
卷积神经网络（Convolutional Neural Networks / CNNs / ConvNets）与普通神经网络非常相似，它们都由具有可学习的权重和偏置常量(biases)的神经元组成。每个神经元都接收一些输入，并做一些点积计算，输出是每个分类的分数，普通神经网络里的一些计算技巧到这里依旧适用。

> `卷积神经网络默认输入是图像，可以让我们把特定的性质编码入网络结构，使是我们的前馈函数更加有效率，并减少了大量参数`。

**具有三维体积的神经元(3D volumes of neurons)**

> `卷积神经网络利用输入是图片的特点，把神经元设计成三个维度 ： width, height, depth(注意这个depth不是神经网络的深度，而是用来描述神经元的)` 。比如输入的图片大小是 32 × 32 × 3 (rgb)，那么输入神经元就也具有 32×32×3 的维度。  
> 一个卷积神经网络由很多层组成，它们的`输入是三维的，输出也是三维的`，有的层有参数，有的层不需要参数。

传统神经网络：  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-bf5ead6dcd365cd531db326c457d8fe0.png)

卷积神经网络  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-588873ea60a24f8ac1a0c52a0a1e012b.png)

# 卷积神经网络组成

## 1、输入层

卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样，二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组。  
由于卷积神经网络在计算机视觉领域应用较广，因此许多研究在介绍其结构时预先假设了三维输入数据，即平面上的二维像素点和 RGB 通道。

> 与其它神经网络算法类似，由于`使用梯度下降算法进行学习，卷积神经网络的输入特征需要进行标准化处理`。具体地，在将学习数据输入卷积神经网络前，需在通道或时间/频率维对输入数据进行归一化，若输入数据为像素，也可将分布 \[0，255\] 的原始像素值归一化至 \[0，1\] 区间。输入特征的标准化有利于提升卷积神经网络的学习效率和表现。  
> 在图像中局部范围内的像素之间联系较为紧密，而距离较远的像素则相关性较弱。因而，`每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。这种模式就是卷积神经网络中降低参数数目的重要神器：局部感受野`。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-f8588dcc5c12284ddce0bb2231cd8319.png)

该层要做的处理主要是对原始图像数据进行预处理，其中包括：

> （1）去均值：把输入数据各个维度都中心化为0，如下图所示，其目的就是`把样本的中心拉回到坐标系原点上`。  
> （2）归一化：`幅度归一化到同样的范围`，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-7cdc149ed49dc1dc2a793aeeb4455e4f.png)  
> （3）PCA/白化：`用 PCA 降维`；`白化是对数据各个特征轴上的幅度归一化`。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-077ad75081d789785b825fd5d375a7e4.png)

## 2、卷积层（Convolutional layer）

卷积层由一组卷积单元（又称"卷积核"）组成，可以把这些卷积单元的集合称为过滤器，每个过滤器都会提取一种特定的特征。

> `卷积层的功能是对输入数据进行特征提取，就是在原始输入上一个小区域一个小区域进行特征的提取`。第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-e74e74c93fd7a2db40e6c0cfdc23c315.png)  
> 当给定一张新图时，CNN并不能准确地知道这些特征到底要匹配原图的哪些部分，所以它会在原图中把每一个可能的位置都进行尝试，相当于把这个feature（特征）变成了一个过滤器。这个用来匹配的过程就被称为卷积操作，这也是卷积神经网络名字的由来。

**局部感知（Local Connectivity）与感受野**

普通神经网络把输入层和隐含层进行“全连接(Full Connected)“的设计。从计算的角度来讲，相对较小的图像从整幅图像中计算特征是可行的。但是，如果是更大的图像（如 96x96 的图像），要通过这种全联通网络的这种方法来学习整幅图像上的特征，从计算角度而言，将变得非常耗时。你需要设计 10 的 4 次方（=10000）个输入单元，假设你要学习 100 个特征，那么就有 10 的 6 次方个参数需要去学习。与 28x28 的小块图像相比较， 96x96 的图像使用前向输送或者后向传导的计算方式，计算过程也会慢 10 的 2 次方（=100）倍。

> 卷积层解决这类问题的一种简单方法是`对隐含单元和输入单元间的连接加以限制：每个隐含单元仅仅只能连接输入单元的一部分`。例如，每个隐含单元仅仅连接输入图像的一小片相邻区域。`每个隐含单元连接的输入区域大小叫神经元的感受野(receptive field)`。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-c7055c66f551e07a6aee25397aee486f.gif)

**直观理解卷积**  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-1f7f315616f024a8711f5234da238b04.png)  
以上图为例： 第一次卷积可以提取出低层次的特征，第二次卷积可以提取出中层次的特征，第三次卷积可以提取出高层次的特征。  
特征是不断进行提取和压缩的，最终能得到比较高层次特征，简言之就是对原式特征一步又一步的浓缩，最终得到的特征更可靠。利用最后一层特征可以做各种任务：比如分类、回归等。

**（1）卷积核**

> 卷积层内部包含多个卷积核，组成卷积核的每个元素都对应一个权重系数和一个偏差量（bias vector），每个卷积核的参数都是通过反向传播算法优化得到的，类似于一个前馈神经网络的神经元（neuron）。卷积核在工作时，会有规律地扫过输入特征，在感受野内对输入特征做矩阵元素乘法求和并叠加偏差量。

**（2）卷积层参数**

卷积层参数包括卷积核大小、步长和填充值，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。

> ● `在达到相同感受野的情况下，卷积核越小，所需要的参数和计算量越小`。具体来说。`卷积核大小必须大于1才有提升感受野的作用`，1排除了。而`大小为偶数的卷积核即使对称地加padding也不能保证输入feature map尺寸和输出feature map尺寸不变`（画个图算一下就可以发现），2排除了。所以`一般都用3作为卷积核大小`。  
> ● `卷积步长定义了卷积核相邻两次扫过特征图时位置的距离`（窗口一次滑动的长度），卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。  
> ● `填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法`。常见的填充方法为按0填充和重复边界值填充（replication padding）。填充依据其层数和目的可分为四类：有效填充（valid padding）、相同填充/半填充（same/half padding）、全填充（full padding）和任意填充（arbitrary padding）。  
> ![比如有这么一个5*5的图片（一个格子一个像素），我们滑动窗口取2*2，步长取2，那么我们发现还剩下1个像素没法滑完，那怎么办呢？](cnn%E7%90%86%E8%A7%A3/1625902940-c860f7e68b982a3303afc0c237ccf0be.png)

一个输出单元的大小有以下三个量控制：depth, stride 和 zero-padding。

> ● `深度`(depth) : 顾名思义，它控制输出单元的深度，也就是filter的个数，连接同一块区域的神经元个数，又名depth column。  
> ● `步幅`(stride)：它控制在同一深度的相邻两个隐含单元，与他们相连接的输入区域的距离。如果步幅很小（比如 stride = 1）的话，相邻隐含单元的输入区域的重叠部分会很多; 步幅很大则重叠区域变少。  
> ● `补零`(zero-padding) ： 我们可以通过在输入单元周围补零来改变输入单元整体大小，从而控制输出单元的空间大小。

**（3）卷积的计算**

> 滤器的每个内核在各自的输入通道上“滑动”，产生每个处理版本。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-54b618902eff9ee004198b917f4acc8e.gif)  
> 然后将每个通道处理的版本汇总在一起形成一个通道。过滤器的内核每个产生一个版本的每个通道，并且整个过滤器产生一个整体输出通道。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-5fc26a52ef2f307fb33274bef144d60a.gif)
>
> 最后，这里有一个偏向的术语。这里偏置项的工作方式是每个输出滤波器都有一个偏置项。到目前为止，偏置被添加到输出通道以产生最终输出通道。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-bffada935fb234024c71c843d240cfa5.gif)

**示例**

> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-86b8582f3a4aa768d0bbaad7bc9c1992.gif)
>
> 上图中：左区域的三个蓝色大矩阵是原式图像的输入，RGB三个通道用三个矩阵表示，大小为 7 \* 7 \* 3；色矩阵周围灰色的框是填充值；粉色矩阵就是卷积层的神经元，这里表示了有两个神经元（w0,w1），因此该卷积层结果的输出深度为2（绿色矩阵有2个）；Bias b0是 Filter W0 的偏置项，Bias b1是 Filter W1 的偏置项；绿色矩阵就是经过卷积运算后的输出矩阵，这里的步长设置为2。  
> 计算过程：
>
> > ①输入是固定的，filter是指定的，因此计算就是如何得到绿色矩阵。在输入矩阵上有一个和 filter 相同尺寸的滑窗，然后输入矩阵的在滑窗里的部分与 filter 矩阵对应位置相乘：  
> > ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-d137f3b97e051fc8447403f73bc88ddf.png)  
> > ② 将3个矩阵产生的结果求和，并加上偏置项，即0+2+0+1=3，因此就得到了输出矩阵的左上角的3：  
> > ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-f7e2fde6b71efac88422940b07b5b407.png)  
> > ③ 让每一个filter都执行这样的操作，变可得到第一个元素：  
> > ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-e0df121de0fd26dc77a6bce24f610aed.png)  
> > ④ 滑动窗口2个步长，重复之前步骤进行计算 ：  
> > ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-f57d2eedb1f309342df9133e5674c0b4.png)  
> > ⑤ 最终可以得到，在2个filter下，卷积后生成的深度为2的输出结果：  
> > ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-5fa6582384874888de2d2a5606561f1a.png)

## 3、激励层（ Activation Function Layer）

激励函数操作通常在卷积核之后，把卷积层输出结果做非线性映射。一些使用预激活（preactivation）技术的算法将激励函数置于卷积核之前。  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-50adf0071e08a9d0677538675e4cc2dd.png)  
CNN采用的激励函数一般为 ReLU(The Rectified Linear Unit / 线性整流函数)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下。  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-695b3537eb2d70cb54b486b98ad8fda9.png)  
在ReLU出现以前，Sigmoid函数和双曲正切函数（hyperbolic tangent）也有被使用。

激励层的实践经验：

>
> ①不要用sigmoid！不要用sigmoid！不要用sigmoid！ 　　  
> ② 首先试RELU，因为快，但要小心点 　　  
> ③ 如果2失效，请用Leaky ReLU或者Maxout 　　  
> ④ 某些情况下tanh倒是有不错的结果，但是很少
>

## 4、卷积池化层(Pooling Layer)

> 池化（pool）即下采样（downsamples），就是将输入图像进行缩小，池化减少像素信息，只保留重要信息，`目的是为了减少特征图`。通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。  
> 池化层夹在连续的卷积层中间， `用于压缩数据和参数的量，减小过拟合`。如果输入是图像的话，那么池化层的最主要作用就是压缩图像。  
> `在卷积层进行特征提取后，输出的特征图会被传递至池化层进行特征选择和信息过滤`。池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量。池化层选取池化区域与卷积核扫描特征图步骤相同，由池化大小、步长和填充控制

**池化层的具体作用**

> ● `特征不变性`，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。  
> ● `特征降维`，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。  
> ● 在一定程度上`防止过拟合`，更方便优化。  
> ![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-f4dcc3b777891b2cf158fa1d25afc34e.png)

**池化层运算的种类**

> 池化的操作也很简单，通常情况下，池化区域是2\*2大小，然后按一定规则转换成相应的值。  
> 池化层进行的运算一般有以下几种：  
> ● `最大池化`（Max Pooling）。取4个点的最大值。这是最常用的池化方法。  
> ● `均值池化`（Mean Pooling）。取4个点的均值。  
> ● 高斯池化。借鉴高斯模糊的方法。不常用。  
> ● 可训练池化。训练函数 ff ，接受4个点为输入，出入1个点。不常用。

最常见的池化层是规模为2\*2， 步幅为2，对输入的每个深度切片进行下采样。每个MAX操作对四个数进行，如下图所示：  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-c3b6aaffc59eb74149e232dfe4e392e4.png)

> ● `池化操作保持深度大小不变`。  
> ● `如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化`。

## 5、全连接层（fully-connected layer）

`两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部`。也就是跟传统的神经网络神经元的连接方式是一样的：  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-ef7b7c162edb9f8502dd18f8c0473e38.png)  
全连接层和卷积层可以相互转换：

> 对于任意一个卷积层，要把它变成全连接层只需要把权重变成一个巨大的矩阵，其中大部分都是0， 除了一些特定区块（因为局部感知），而且好多区块的权值还相同（由于权重共享）。  
> 相反地，对于任何一个全连接层也可以变为卷积层。比如，一个 的全连接层，输入层大小为 ，它可以等效为一个 的卷积层。换言之，我们把 filter size 正好设置为整个输入层大小。

## 6、输出层

卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数（softmax function）输出分类标签。在物体识别（object detection）问题中，输出层可设计为输出物体的中心坐标、大小和分类。在图像语义分割中，输出层直接输出每个像素的分类结果。

## 卷积神经网络架构

常见的卷积神经网络架构是这样的：

> ```css
> INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC
> ```
>
> `堆叠几个卷积和整流层，再加一个池化层，重复这个模式知道图片已经被合并得比较小了，然后再用全连接层控制输出`。  
> 上述表达式中 ? 意味着0次或1次，通常情况下：N >= 0 && N <= 3, M >= 0, K >= 0 && K < 3。

比如你可以组合出以下几种模式：

> ● INPUT -> FC, 实现了一个线性分类器， 这里 N = M = K = 0 ；  
> ● INPUT -> CONV -> RELU -> FC ；  
> ● INPUT -> \[CONV -> RELU -> POOL\]\*2 -> FC -> RELU -> FC；Here we see that there is a single CONV layer between every POOL layer.  
> ● INPUT -> \[CONV -> RELU -> CONV -> RELU -> POOL\]\*3 -> \[FC -> RELU\]\*2 -> FC ；Here we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper networks, because multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation.

## 卷积网络的设计公式

**（1）卷积层的填充（padding）公式**  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-276028155da05ee785cfff7683981b4c.png)  
**（2）计算卷积层输出大小的公式**  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-5314772e5bb9140ada68b022b96c6bcd.png)  
**（3）计算卷积后参数个数**  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-8708ced985a296be4edf71d4b840fb85.png)  


**（4）计算池化层输出大小的公式**  
![在这里插入图片描述](cnn%E7%90%86%E8%A7%A3/1625902940-9525c8c6e358d974c2553923a15f9fa8.png)



