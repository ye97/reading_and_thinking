---
title: BN层
categories:

  - 论文
  - 深度
description: <read more ...>
mathjax: true
permalink_defaults: 'category/:title/'
date: 2021-07-11 15:09:16
urlname:
tags:
---

Batch Normalization是由google提出的一种训练优化方法。参考论文：Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift  
网上对BN解释详细的不多，大多从原理上解释，没有说出实际使用的过程，这里从what, why, how三个角度去解释BN。

## **What is BN**

Normalization是数据标准化（归一化，规范化），Batch 可以理解为批量，加起来就是批量标准化。  
先说Batch是怎么确定的。在CNN中，Batch就是训练网络所设定的图片数量batch\_size。

Normalization过程，引用论文中的解释：  
![这里写图片描述](BN%E5%B1%82/1625987603-7836ddb5560b4781dfeb2427a0b5c077.png)  
输入：输入数据x1…xm（这些数据是准备进入激活函数的数据）  
计算过程中可以看到,  
1.求数据均值  
2.求数据方差  
3.数据进行标准化（个人认为称作正态化也可以）  
4.训练参数γ，β  
5.输出y通过γ与β的线性变换得到新的值  
_**在正向传播的时候，通过可学习的γ与β参数求出新的分布值**_

_**在反向传播的时候，通过链式求导方式，求出γ与β以及相关权值**_  
![这里写图片描述](BN%E5%B1%82/1625987603-db00570ac1ceec3c981c1f556100de3f.png)

## **Why is BN**

解决的问题是梯度消失与梯度爆炸。  
关于_梯度消失_，以sigmoid函数为例子，sigmoid函数使得输出在\[0,1\]之间。  
![这里写图片描述](BN%E5%B1%82/1625987603-e017f9fa46363ca5d706f7c0a9d26eb6.png)  
事实上x到了一定大小，经过sigmoid函数的输出范围就很小了，参考下图  
![这里写图片描述](../../../../Download/mx-wc/BN%25E5%258E%259F%25E7%2590%2586/2021-07-11-1625987603/assets/1625987603-098a2fede510cefb557bfb59c5236202.png)  
如果输入很大，其对应的斜率就很小，我们知道，其斜率（梯度）在反向传播中是权值学习速率。所以就会出现如下的问题，  
![这里写图片描述](../../../../Download/mx-wc/BN%25E5%258E%259F%25E7%2590%2586/2021-07-11-1625987603/assets/1625987603-a83c02c984beb5ecf2e385cb9178bd69.png)  
在深度网络中，如果网络的激活输出很大，其梯度就很小，学习速率就很慢。假设每层学习梯度都小于最大值0.25，网络有n层，因为链式求导的原因，第一层的梯度小于0.25的n次方，所以学习速率就慢，对于最后一层只需对自身求导1次，梯度就大，学习速率就快。  
这会造成的影响是在一个很大的深度网络中，浅层基本不学习，权值变化小，后面几层一直在学习，结果就是，后面几层基本可以表示整个网络，失去了深度的意义。

关于_梯度爆炸_，根据链式求导法，  
第一层偏移量的梯度=激活层斜率1x权值1x激活层斜率2x…激活层斜率(n-1)x权值(n-1)x激活层斜率n  
假如激活层斜率均为最大值0.25，所有层的权值为100，这样梯度就会指数增加。


为什么BN能加速训练收敛
		Internal Covariate Shift
提到训练收敛的问题，需要先解释一个现象，叫做内部协变量偏移(Internal Covariate Shift)，对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。

固定输入分布
而BatchNorm的基本思想就是，能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了“Internal Covariate Shift”问题了。所以就有了BN中的数据标准化处理，BN 其实就是在做 feature scaling，而且它的目的也是为了在训练的时候避免这种 Internal Covariate Shift 的问题。

batch normalization
顺着这个思路想下去，如果要限制到一个固定的输入，那应该是什么样子。BN选择了normalization，也就是一种白化操作，这个很好理解，我们在原始图像输入到网络之前都会做减均值，除标准差的操作，这个操作有时单独做，有时和第一层卷积组合起来做，但是不管哪种都必须要有的。图像处理中对输入图像进行白化（Whiten）可以有效加快神经网络的收敛，而对多层网络中的每一层输出做白化，就是我们看到的在CNN中加入BN层的样子。

$\gamma$&$ \beta$
但是到这里还没有结束，简单直接白化引入了太多的人工设计，使输入强行归一化，对于原始图像来说这样直接处理还行，但是越是到了网络的深层，CNN提取出来的特征就越是无法解释，那么同样的白化操作就变得不合理，所以BN在白化之后引入了两个参数控制白化的程度，它刚刚好使白化的逆过程，而且，这两个参数是参与训练的，那么一种极端的可能就是，白化如果对结果没有作用，那么$\gamma$&$ \beta$很有可能将白化后的结果恢复成原来的样子。

## **How to use BN**

先解释一下对于图片卷积是如何使用BN层。  
![这里写图片描述](BN%E5%B1%82/1625987603-5cb9098c33de75ad7a1852b42e70b523.gif)  
这是文章卷积神经网络CNN（1）中5x5的图片通过valid卷积得到的3x3特征图（粉红色）。这里假设通道数为1，batch为4，即大小为\[4,1,3,3\] (n,c,h,w)。特征图里的值，作为BN的输入，这里简化输出只有一个channel，也就是这一个4x3x3个数值通过BN计算并保存均值与方差，并通过当前均值与方差计算归一化的值，最后根据γ,β以及归一化得值计算BN层输出。

**这里需要着重说明的细节：**  
网络训练中以batch\_size为最小单位不断迭代，很显然，新的batch\_size进入网络，由于每一次的batch有差异，实际是通过变量，以及滑动平均来记录均值与方差。训练完成后，推断阶段时通过γ, β，以及记录的均值与方差计算bn层输出。

结合论文中给出的使用过程进行解释  
![这里写图片描述](BN%E5%B1%82/1625987603-5d7bce04f0cdd3a7b81e0bdeff90730c.png)  
输入：待进入激活函数的变量  
输出：  
1.对于K个激活函数前的输入，所以需要K个循环。每个循环中按照上面所介绍的方法计算均值与方差。通过γ,β与输入x的变换求出BN层输出。  
2.在反向传播时利用γ与β求得梯度从而改变训练权值（变量）。  
3.通过不断迭代直到训练结束，得到γ与β，以及记录的均值方差。  
4.在预测的正向传播时，使用训练时最后得到的γ与β，以及均值与方差的无偏估计，通过图中11:所表示的公式计算BN层输出。  
至此，BN层的原理与使用过程就解释完毕，给出的解释都是本人觉得值得注意或这不容易了解的部分，如有錯漏，请指正。