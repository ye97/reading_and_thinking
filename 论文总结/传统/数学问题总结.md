---
title: 数学问题总结
categories:

  - 论文
  - 数学
description: <read more ...>
mathjax: true
permalink_defaults: 'category/:title/'
date: 2021-05-23 20:03:45
urlname:
tags:
---

# 贝叶斯公式

![img](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/v2-68331cf4e8993062c31d22d61eee924f_720w.jpg)





![image-20210718195308919](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/image-20210718195308919.png)

；表示β是一个参数，就是这个x的概率密度是一个β函数

|表示限定后者时，前者概率

，表示两者的联合密度，也就是两者X取到x和O取到o时候的密度函数

# em算法

在介绍EM算法之前，我们先来回归K-means算法。

K-means其实非常简单，就是设定K个中心，然后不断的调整这k个类别的**边界**和**中心**。

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-4955d6550120dd7222e00667514afd25.jpg)

整个模型有两套参数，分别是负责决定中心的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-d4d86e11553e0c2c73e6e5b8200220e5.svg) ，因为有K个中心，所以一共有K个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cea00341468ee5ebbd8bbda853b43afe.svg) 。

另外则是决定类别边界的集合 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-96cd0c66831377044bebc676cc938856.svg) ， ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-96cd0c66831377044bebc676cc938856.svg) 可以看作一个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-9d89e5a0a435a8243ed90d2942dbee08.svg) 矩阵，其中每一个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-9f6e16b0acbaff038996ef565329d3f8.svg) 满足 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-3059ca90ae8f635a25fe3c7376252c85.svg) 。

实际上，在K-means这个简单的例子中，这个 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-96cd0c66831377044bebc676cc938856.svg) 即代表了对一个叫做**潜变量**的东西的**估计**，又代表了一个被成为**Responsibility**的东西，我们会在之后再次看见它们。

![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-96cd0c66831377044bebc676cc938856.svg) 这个矩阵每一列只有一个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-227e8e885aa645310c5e4bbc05c50091.svg) ，其余则都是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-b0002d5deeecc7acffc2335d8b8c7927.svg) 。实际上就是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-390824601cf6d50933744af0e4ccf208.svg) 个one-hot向量写在一起。

这个集合 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-96cd0c66831377044bebc676cc938856.svg) 用来决定每个数据**属于哪个类别**。

于是，K-means算法的优化目标是这个：

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-a3a0875848066cb86b992b09941a3b21.svg)

我们观察这个式子，我们需要同时优化 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-7367159203b7140e9ca03aee77dff7c5.svg) 和 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cea00341468ee5ebbd8bbda853b43afe.svg) ，但是直接求梯度并不容易。

实际上，K-means的算法要简单的多，并且可以看作是EM算法的简化版本。

我们先随便初始化一组 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cea00341468ee5ebbd8bbda853b43afe.svg) ，然后调整 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-07950c80cef64fc7d0f7a5f3e0a78238.svg)

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-7995fb16470ab8b4c8048a13775e1b62.svg)

这样 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-487efbc22f7a0514b4b7934844e7bcc2.svg) 就变成了简单的二次型了，可以直接求导

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-dd0d34411913c837ff9320d74290b729.svg)

计算得到的中心就是这类数据的均值

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-63b56141ae450f35c25494314dcd9f1b.svg)

重复上面的过程，不停的计算新的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-7367159203b7140e9ca03aee77dff7c5.svg) 和 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-3255aa7a3988b23abd1e262e12deab6b.svg) ，直至收敛。

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-298e2203c371cd57b1dd8b3517aea0cf.jpg)

* * *

这个例子比较简单，我们再看一个例子，是EM在高斯混合模型上的应用。

高斯混合模型是这样的

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-1344b324519607b12fdbe896e2117237.svg)

可以看到它是多个高斯模型的加权和，其中权重是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-3c2795cc648d0d149e5adf5a17c5edae.svg) 。

注意，这里的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-3c2795cc648d0d149e5adf5a17c5edae.svg) 和前面K-means中的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-7367159203b7140e9ca03aee77dff7c5.svg) 可不太一样，因为 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-7367159203b7140e9ca03aee77dff7c5.svg) 是针对每一个样本 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-ce59f55893efd960f1dbdfcd5c192a96.svg) 的每一个类别 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-9d4a1163d2eb1bb72228d1279049c2d8.svg) 而言的，而 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-3c2795cc648d0d149e5adf5a17c5edae.svg) ，显而易见，是对整个模型而言的。

可以理解为K-means是**硬**的，在某个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cea00341468ee5ebbd8bbda853b43afe.svg) 支配的范围内，某个样本 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cf23792eae6195c34e987a3839b5548f.svg) 属于这个类别 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-9d4a1163d2eb1bb72228d1279049c2d8.svg) 的概率就是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-227e8e885aa645310c5e4bbc05c50091.svg) ，而高斯混合模型是**软**的，我们用Responsibility ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-87fad3893ac7411017ecb4d18250bfd4.svg) 表示这个概率，我们在后面会看到这一点。

也就是说，当你决定了 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-1b8e4b193ff691847aa901853800448d.svg) 之后，模型就唯一确定了，例如下面的图

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-7aaf377ec5e656c53e14f96cc40275d3.jpg)

  

注意这个高斯混合模型是有三个峰值的，他们的均值和方差都不太一样，而他们通过权重 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-3c2795cc648d0d149e5adf5a17c5edae.svg) 结合到一起，形成了红色的线。

在上面的例子中，K=3。

我们要做的是，对于一个已经给定的数据集，找出对应的极大似然的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-1b8e4b193ff691847aa901853800448d.svg) 。

然而，对应的对数似然长成下面的样子

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-b58417122ebc270b851393abc47cdbbd.svg)

这个形式比单个的高斯分布糟糕的多，因为它没有二次型了，这意味这我们需要用很多计算量来进行梯度下降。

同时，直接用梯度下降解决这个问题还有本质上的缺陷。

设想一下，如果某一个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-3cf117fc436eb83025200a8eda3eba13.svg) ，这里的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-5dcdf3d3e83b8c95f38f7ac2934baf1b.svg) 指的就是随便的一个样本。

那么，当对应的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-f617d315d82f3f2f4acc9d6d8edeb5b6.svg) 趋近于 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-b0002d5deeecc7acffc2335d8b8c7927.svg) 的时候， ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-26737275963fd2577e35808385e441a8.svg) 就可能趋紧正无穷，而这样的极大是病态的，因为它并没有实质上解决任何问题。

就像下图这样

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-6e5666feeb29b74396126fbcbde4c5f0.jpg)

因此，我们还是用EM算法来解决这个问题。

在K-means中，我们先是随机初始化了参数 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cea00341468ee5ebbd8bbda853b43afe.svg) ，而现在，我们有了更多的参数，当然我们也可以随机初始化他们。也就是初始化 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-1b8e4b193ff691847aa901853800448d.svg) 。

现在，我们需要计算一个类似 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-7367159203b7140e9ca03aee77dff7c5.svg) 的东西，实际上，在高斯混合中，这个东西是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-87fad3893ac7411017ecb4d18250bfd4.svg) ，其中 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-524eb39ab25a7a40826f20cecea0aa82.svg) 是**潜变量**，潜变量是EM算法的核心。

什么是潜变量呢，我们认为每一个样本 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cf23792eae6195c34e987a3839b5548f.svg) 的观测都是不完整的。举个例子，我们用三个高斯分布组成的混合高斯分布定义一个概率分布，然后进行采样（相当于观测）。

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-22ab74a654570c213a52aaaec174ba22.jpg)

那么，我们说完整的观测其实是上图左面这样的，也就是既包括样本的**值**，也包括样本的**类别**。

想象一下，如果我们已经知道了这些样本都分别是从哪个高斯分布的**成分**（指组成混合高斯的单个高斯）中采样的，我们不是可以直接得到完整的高斯混合模型吗？

因为这只要分别做三次高斯建模，然后根据不同类别的数量设置一下 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-3c2795cc648d0d149e5adf5a17c5edae.svg) 就好了。

我们管这个类别叫做潜变量 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-40806970f4b07005d78060fcad9259e5.svg) 。在这里， ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-40806970f4b07005d78060fcad9259e5.svg) 应该是一个三维的向量，表示这个样本是属于哪个类别的。

但是要注意， ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-40806970f4b07005d78060fcad9259e5.svg) 的作用就和K-means中的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-44940b6e7bcf152b4298929deaef2e5f.svg) ，也就是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-27768ce74698b1e78ce8deda26a30c7c.svg) 组成的向量，并不太一样。

因为 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-40806970f4b07005d78060fcad9259e5.svg) 是真实存在的，是固定的，只是我们观测不到，而K-means中的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-44940b6e7bcf152b4298929deaef2e5f.svg) ，是我们预测出来的一个中间结果。

但是，我们不能观测 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-40806970f4b07005d78060fcad9259e5.svg) ，也就是我们观测到的样本实际上是上图中中间的那种，因此问题变得棘手了。

就像在K-means中可以先根据随便初始化的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cea00341468ee5ebbd8bbda853b43afe.svg) 胡乱算一个 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-7367159203b7140e9ca03aee77dff7c5.svg) 一样，我们也可以先胡乱算出这个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-87fad3893ac7411017ecb4d18250bfd4.svg) 。

我们都说了 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-87fad3893ac7411017ecb4d18250bfd4.svg) 只是一个中间结果，这意味着它是不是最精确的这件事并不是很重要，实际上，我们通过迭代的计算逐渐使它和 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-40806970f4b07005d78060fcad9259e5.svg) 靠拢。

我们定义 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-b3a7226e74020d30c1be707fd7f34daa.svg) ，为了方便起见省略了 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-59bb8c2069481de6ad51cb5785cfde4e.svg) 的下角标。

那么有

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-5e7a0d2cac6a2e2a81ed0d0147d81e5c.svg)

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-87fad3893ac7411017ecb4d18250bfd4.svg) 其实也会很自然的在接下来的推导中出现。

我们刚才说高斯混合模型的对数似然是

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-b58417122ebc270b851393abc47cdbbd.svg)

对 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cea00341468ee5ebbd8bbda853b43afe.svg) 求导得到

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-983aac71bc7332e42db369db2f963ae5.svg)

于是

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-4f67c1c28a283baa594f113f8b68d427.svg)

其中

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-59ea899148699569bca0348865f48b7e.svg)

同样的 ，对 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-27a603cf406473ec5be1e7a8b70afe5f.svg) 求导，令导数为0，得到

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-38ac074f0b48daedce977785b3efb980.svg)

接下来对 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-3c2795cc648d0d149e5adf5a17c5edae.svg) 求导，注意，这里的 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-3c2795cc648d0d149e5adf5a17c5edae.svg) 并不能直接求导，因为它是满足和为1的约束的，我们使用拉格朗日乘子法

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-135b1819c22efbcde3b8395932ab19b4.svg)

求导得到

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-418df718ac1c17d3af52e18f868ba2eb.svg)

于是

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-6b6894c5f79a060972313f48e8ddc931.svg)

到此为止，其实我们就已经完成了高斯混合模型的计算。

我们首先初始化一组 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-1b8e4b193ff691847aa901853800448d.svg) ，接着用这个 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-1b8e4b193ff691847aa901853800448d.svg) 计算 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-87fad3893ac7411017ecb4d18250bfd4.svg) 。

有了 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-87fad3893ac7411017ecb4d18250bfd4.svg) 之后，就可以很自然的计算新的 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-1b8e4b193ff691847aa901853800448d.svg) 。

不停重复上面的过程直至收敛即可。

那这里的潜变量起到了什么作用呢？

实际上，就是起到了让一个整体不是凸函数的对数似然，近似为了一个局部具有凸性的函数，然后不停的调整，直到两个函数的极大值点重合。

* * *

我们来看看EM算法的统一形式，在这里我们就能看到EM算法为什么叫EM(Expectation Maximization)算法了。

首先，我们要做的是找到参数 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-4e7be192e8484f9bac4896bb4ce7bc9b.svg) 使得下面的对数似然最大

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-ada293ce951ad3148495e4e650474acc.svg)

这个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-4e7be192e8484f9bac4896bb4ce7bc9b.svg) 在K-means中就是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-3255aa7a3988b23abd1e262e12deab6b.svg) ，在高斯混合中就是 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-1b8e4b193ff691847aa901853800448d.svg) 。

但是往往这个对数似然很难直接求导，因此，我们引出潜变量 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-6bcb777844b2586a9ab5d918f09a6316.svg) 。

我们把一对 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cd825c38944d8109bf777a772a699554.svg) 成为完整的观测，而相应的只有 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cf23792eae6195c34e987a3839b5548f.svg) 则是不完整的观测。

所有 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-cf23792eae6195c34e987a3839b5548f.svg) 组成矩阵 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-2254783cb8ef218cc559f52fa9c4499d.svg) ，所有的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-e0cec55ef27cceb935e5838e5029338c.svg) 组成矩阵 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-f7c05b29dd2e89099095cdfb26434907.svg) 。

于是

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-e2b4ab4995673d0123008a52827e801e.svg)

我们假设 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-1587278c4f547ff222f05061eeb5d16e.svg) 要比 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-ada293ce951ad3148495e4e650474acc.svg) 容易求导的多（这是EM的大前提）。

那么，我们的方法是先随机初始化一个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-4e7be192e8484f9bac4896bb4ce7bc9b.svg) ，称这个 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-4e7be192e8484f9bac4896bb4ce7bc9b.svg) 为 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-695396e00da4155f9358693b8b7b59b0.svg) ，那么我们可以计算得到 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-f7c05b29dd2e89099095cdfb26434907.svg) ，也就是说我们有了概率 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-9608356b1ef4a6df58e31225e4d2cb4a.svg) 。

现在，我们优化

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-742772ab44f8fbd446b8bb2abd76ded1.svg)

注意这里的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-695396e00da4155f9358693b8b7b59b0.svg) 是定值，只需要优化 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-4e7be192e8484f9bac4896bb4ce7bc9b.svg) 就可以了。

于是得到了

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-9f3149720e00513e7e604cb6ce086c51.svg)

接下来把 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-b565fddeeba974efc7ef495828f61cdb.svg) 换成 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-c6dd20bbbe1149c5a89d98c85b8046ad.svg) ，重复上面的过程直至收敛。

那这样的方法为什么叫做EM算法呢，让我们回到高斯混合模型的例子中。

对于完整的数据 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-021cbc5da08c881e78397b161850ddf6.svg) ，对应的似然函数是

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8b1b55bbcd715b16a3e12c53ffb8d223.svg)

我们刚才说到，对于完整观测，对数似然是很容易求导的，实际上

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-d455338bcb83b4c0801ccab2dc94a398.svg)

这是很简洁的形式，因为高斯分布直接和对数结合，这会消掉高斯分布的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-4f5f7fceef3333a72aeeb0a7d5cd7b00.svg) 函数。

优化这个式子我们可以很容易的得到 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-1b8e4b193ff691847aa901853800448d.svg) 。

实际上，换一种角度，我们计算 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-c641037d820af73e1ae575fde2442f11.svg) 的期望

首先，根据 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-021cbc5da08c881e78397b161850ddf6.svg) 的联合概率分布容易得到 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8bd1b9d9007066189f016954a9759a79.svg) 的边缘概率分布

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-a3fa61259f58890ee1f2857ac03d09ff.svg)

这是因为

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8bfc3da128e959578ffb8315531f10b9.svg)

其中 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-a9e758cd53bc56e9f30dd4849a1942f4.svg) 是给定的，因此 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-064a250f77fcab3d25bf1924295a9b05.svg) 是定值。

接下来我们计算 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-da8525501c0d9d326d65c17c6b9590eb.svg) 的期望

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-877c08de9484facd807ad788747f7e9f.svg)

于是

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-6df5918a7155bcafccf44194adc4e3f2.svg)

这不就是我们要最大化的式子吗？

所以，我们根据 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-695396e00da4155f9358693b8b7b59b0.svg) 求出 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-f7c05b29dd2e89099095cdfb26434907.svg) 被成为E步，因为这步是在求一个期望。

而重新计算新的 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-4e7be192e8484f9bac4896bb4ce7bc9b.svg) 的过程为成为M步，因为这步是在极大化这个期望。

这就是EM算法了。

* * *

接下来，我们可以再深入的了解一下EM算法，从而证明EM的确可以做到**极大似然**。

注意到有

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-f51b3cc3a09e778c620c5746ae96271c.svg)

于是

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-e40cb64775c656c5e5e34bdb1b57f001.svg)

引入 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) ，得到

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-a011d9a12c52c9d15b006a795fb4814b.svg)

同乘 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) 并积累和（就是对 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-89cb4895982acfb0cb51ff28bff85eaa.svg) 根据 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) 求期望）

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8b02ebd0d34e083526aec83196014c17.svg)

注意到 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-873e9015c3d9f6e920185a17dddc3691.svg) 与 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) 无关，于是有

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-b60b95554417e6cad53f2c89e91b395b.svg)

于是

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-44ac3ef184012f4014d81f64b4a34970.svg)

其中

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-4a8cc2ac6907f8ed4f6c3bf9fd040d85.svg)

这个式子相当有趣。

等式左边的就是我们的优化目标，而右边的则是一个泛函 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-bde749572e7e0ffde1fd0db250db5699.svg) 与一个散度 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-3603bf16fef6ea90b2c5308161d929b1.svg) 的和。

首先注意到，KL散度是恒正的，于是有

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-1a79f3cf4936024a6be8fd0a3902dbfd.svg)

这说明 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-79cdb06a8252526cc5be4e2c7f69051b.svg) 是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-97ab22706447d00e0a1d6fdf3a04f238.svg) 的一个下届估计。

![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) 是什么呢，我们认为 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) 是潜变量的分布，当然这不是真实的分布，真实的分布谁也不知道，这相当于一个估计。

也就是说我们希望 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) 能很好的体现潜变量的分布。

当我们随机初始化了一组参数 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8c95d176c41665297739ceffa3e3a54f.svg) 了之后，我们有

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-ea6eed73e84db272bd9055c1d62a52b8.svg)

这当然不是一个很好的 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) ，因为 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8c95d176c41665297739ceffa3e3a54f.svg) 是随机初始化的。

接下来，我们根据 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) 调整 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-6c51d5acb2c865eeefdbafe9987673e3.svg) ，根据这个新的 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-6c51d5acb2c865eeefdbafe9987673e3.svg) ，我们可以计算得到更好的 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-41076debbf63487b4352b4cfbf1e0c60.svg) 。

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-5710c4e0cdd656d75a6f23d1f5c8b32e.svg) 的关系是这样的

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-fb578a2da25e78843d27dd1bf059103f.jpg)

我们来看看当我们进行EM的时候，到底发生了什么。

首先，我们初始化一个 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8c95d176c41665297739ceffa3e3a54f.svg) ，接下来进行E步，也就是得到一个新的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-bcfcf30a1f98d18f5f005862e59af11e.svg) 。

仔细观察这个式子

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-44ac3ef184012f4014d81f64b4a34970.svg)

等式左边是和 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-bcfcf30a1f98d18f5f005862e59af11e.svg) 无关的，实际上，我们说 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-79cdb06a8252526cc5be4e2c7f69051b.svg) 是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-97ab22706447d00e0a1d6fdf3a04f238.svg) 的一个下届估计，而E步则是**让这个下届估计尽可能准确**。

如何让下届估计尽可能准确呢，实际上，我们只需要让 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8f6c49b56d54b52b47e2742620fd36f9.svg) 等于0就可以了。

注意到

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-a08f6fab8fb387d1c4ed57e53b30c121.svg)

那么，让 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8f6c49b56d54b52b47e2742620fd36f9.svg) 最小即是令

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-27964e15ff36ece0e3c3732841e641ba.svg)

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-5a8195e8d969bda4a43cfca6604891cf.jpg)

接下来，既然这个下届估计足够精确了，我们有信心通过最大化这个下届来最大化原来的似然了。

于是我们最大化 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-79cdb06a8252526cc5be4e2c7f69051b.svg) ，这时 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-dd4fc2619664c201d2f946dce7306adc.svg) 已经是一个定值了。

我们把 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-dd4fc2619664c201d2f946dce7306adc.svg) 换成 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-9ad16f58ec95d0369ea7b201a478dce2.svg)

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-55d1221f92d68ebfd1cb7c0d639e3266.svg)

所以，最大化 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-79cdb06a8252526cc5be4e2c7f69051b.svg) 实际上就是在最大化 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-83d48b94fe7a4623755f14b969ec3272.svg) ，这和我们前面的算法一致。

而当我们得到了新的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-c6dd20bbbe1149c5a89d98c85b8046ad.svg) 之后，下届估计又不准确了，也就是说 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8f6c49b56d54b52b47e2742620fd36f9.svg) 又出现了。

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-f641483cf34ac488f1443715ddf23680.jpg)

我们不停的重复上面的过程，可以保证每进行了一轮E步和M步之后，整体的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-ada293ce951ad3148495e4e650474acc.svg) 都是在增长的，这保证了算法可以找到一个局部最优。

什么时候停止增长呢？

直到 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-27964e15ff36ece0e3c3732841e641ba.svg) 这个等式恒成立，也就是 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-8f6c49b56d54b52b47e2742620fd36f9.svg) 彻底消失的时候。

通过观察下面的 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-ada293ce951ad3148495e4e650474acc.svg) 和 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-79cdb06a8252526cc5be4e2c7f69051b.svg) 的图像，可以更好的理解上面的过程。

![](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-c7bde13c38678c460a4fc5cb1c713048.jpg)

上面的图也很好的说明了我们之前说的

> 实际上，就是起到了让一个整体不是凸函数的对数似然，近似为了一个局部具有凸性的函数，然后不停的调整，直到两个函数的极大值点重合。

* * *

最后，我们说明我们可以把贝叶斯的视角引入到EM算法中来，也就是给出一个 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-6c51d5acb2c865eeefdbafe9987673e3.svg) 的先验概率 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-93cf508700517487d4dff340b5052d59.svg) 。

那么 ![[公式]](../../../../Download/mx-wc/em/2021-07-18-1626612782/assets/1626612782-6c51d5acb2c865eeefdbafe9987673e3.svg) 的后验概率可以表示为

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-c56e2e0dd3ea95e39d7788d35c86d8d4.svg)

于是有

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-4c8517da818610042105490710e0292a.svg)

我们之前说

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-44ac3ef184012f4014d81f64b4a34970.svg)

那么有

![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-a613cf936e2fc6bcf24b5786eecaf10a.svg)

我们不需要改变E步的操作，只需要轻微调整M步的操作，也就是同时优化 ![[公式]](%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/1626612782-92449e028dba7bc61a85f65b84ee7371.svg) ，让它们的和最小。

